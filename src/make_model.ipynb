{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms as tf\n",
    "\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "# from sklearn import preprocessing\n",
    "from skimage import transform\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from imgaug import augmenters as iaaot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../networks/')\n",
    "from Att_Net import Att_Net\n",
    "# from help_functions import *\n",
    "\n",
    "# #function to obtain data for training/testing (validation)\n",
    "# from extract_patches import get_data_training\n",
    "# sys.path.insert(0, '../lib/networks/')\n",
    "\n",
    "from preprocessing import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train images/masks shape:\n",
      "(20, 1, 565, 565)\n",
      "train images range (min-max): 0.0 - 1.0\n",
      "train masks are within 0-1\n",
      "\n",
      "patches per full image: 9500\n",
      "\n",
      "train PATCHES images/masks shape:\n",
      "(190000, 1, 48, 48)\n",
      "train PATCHES images range (min-max): 0.00784313725490196 - 1.0\n",
      "(190000, 1, 48, 48)\n",
      "48 48 1\n",
      "......DONE......\n",
      "masks shape:  (190000, 2304)\n"
     ]
    }
   ],
   "source": [
    "train_img, label_img = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((190000, 1, 48, 48), (190000, 2304))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img.shape, label_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_subimgs = 190000\n",
    "indices = list(range(N_subimgs))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_size = 1/10\n",
    "split = np.int_(np.floor(val_size * N_subimgs))\n",
    "\n",
    "train_idxs = indices[split:]\n",
    "val_idxs = indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,preprocessed_images, train=True, label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.images = preprocessed_images\n",
    "        if self.train:\n",
    "            self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        img = np.zeros_like(image, dtype=np.float32)\n",
    "        \n",
    "        img += image\n",
    "        label = None\n",
    "        if self.train:\n",
    "            label = self.label[idx]\n",
    "#             msk = np.zeros((2,48,48), dtype=np.long)\n",
    "#             msk[1] = label\n",
    "#             msk[0] = 1-label\n",
    "            \n",
    "#             msk += label\n",
    "            return (img, label)\n",
    "        return img\n",
    "\n",
    "eye_dataset_train = eye_dataset(train_img[train_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[train_idxs])\n",
    "\n",
    "eye_dataset_val = eye_dataset(train_img[val_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[val_idxs])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=eye_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=eye_dataset_val, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Att_Net(32)\n",
    "model.cuda()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "                             weight_decay=.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, \n",
    "                                                 milestones=[400,600,700], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Att_Net(\n",
       "  (input_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (block1): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (block2_res): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (block3): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (block3_res): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (mid): MultiHeadAttention(\n",
       "    (q): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (k): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (v): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "  (up2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (up2_1): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (up1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  (up1_1): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (drop): Dropout2d(p=0.5)\n",
       "  (out): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.997, affine=True, track_running_stats=True)\n",
       "    (4): ReLU6(inplace)\n",
       "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (out_): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.0001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "#                              weight_decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    total = 0.0\n",
    "    predicted = torch.exp(out)\n",
    "    size = predicted.shape[0]*predicted.shape[2]\n",
    "    pred = torch.argmax(predicted.data, dim=1)\n",
    "    total += torch.sum(pred == labels.data)\n",
    "    return total.cpu().detach().numpy()/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1\n",
      "Training loss:....... 0.13604353376926331\n",
      "Validation loss:..... 0.13128396630387515\n",
      "\n",
      "Training accuracy:... 0.9476134281468452\n",
      "Validation accuracy.. 0.949282039193606\n",
      "NEW BEST Loss: 0.13128396630387515 ........old best:99999\n",
      "\n",
      "NEW BEST Acc: 0.949282039193606 ........old best:-99999\n",
      "\n",
      "EPOCH:  2\n",
      "Training loss:....... 0.1333404085041208\n",
      "Validation loss:..... 0.13475874126559556\n",
      "\n",
      "Training accuracy:... 0.9484677259918175\n",
      "Validation accuracy.. 0.9470627355420108\n",
      "\n",
      "EPOCH:  3\n",
      "Training loss:....... 0.13114482527152596\n",
      "Validation loss:..... 0.12777765203104277\n",
      "\n",
      "Training accuracy:... 0.949163026631122\n",
      "Validation accuracy.. 0.9504525108840973\n",
      "NEW BEST Loss: 0.12777765203104277 ........old best:0.13128396630387515\n",
      "\n",
      "NEW BEST Acc: 0.9504525108840973 ........old best:0.949282039193606\n",
      "\n",
      "EPOCH:  4\n",
      "Training loss:....... 0.1292952556179803\n",
      "Validation loss:..... 0.13054470226218806\n",
      "\n",
      "Training accuracy:... 0.9497669279071137\n",
      "Validation accuracy.. 0.9486657932092502\n",
      "\n",
      "EPOCH:  5\n",
      "Training loss:....... 0.12779354241057606\n",
      "Validation loss:..... 0.12541814155951894\n",
      "\n",
      "Training accuracy:... 0.9502401512370071\n",
      "Validation accuracy.. 0.9511039831338767\n",
      "NEW BEST Loss: 0.12541814155951894 ........old best:0.12777765203104277\n",
      "\n",
      "NEW BEST Acc: 0.9511039831338767 ........old best:0.9504525108840973\n",
      "\n",
      "EPOCH:  6\n",
      "Training loss:....... 0.12642158235808065\n",
      "Validation loss:..... 0.1254923587997353\n",
      "\n",
      "Training accuracy:... 0.9506867822727406\n",
      "Validation accuracy.. 0.9510948365047215\n",
      "\n",
      "EPOCH:  7\n",
      "Training loss:....... 0.12528972818466003\n",
      "Validation loss:..... 0.12599566523675565\n",
      "\n",
      "Training accuracy:... 0.9510449613526387\n",
      "Validation accuracy.. 0.9507122927668716\n",
      "\n",
      "EPOCH:  8\n",
      "Training loss:....... 0.12414444885133001\n",
      "Validation loss:..... 0.12399640237843548\n",
      "\n",
      "Training accuracy:... 0.9514084649101863\n",
      "Validation accuracy.. 0.9517110531311378\n",
      "NEW BEST Loss: 0.12399640237843548 ........old best:0.12541814155951894\n",
      "\n",
      "NEW BEST Acc: 0.9517110531311378 ........old best:0.9511039831338767\n",
      "\n",
      "EPOCH:  9\n",
      "Training loss:....... 0.12313557722923642\n",
      "Validation loss:..... 0.12419598725207326\n",
      "\n",
      "Training accuracy:... 0.951726676506548\n",
      "Validation accuracy.. 0.9518854849597581\n",
      "NEW BEST Acc: 0.9518854849597581 ........old best:0.9517110531311378\n",
      "\n",
      "EPOCH:  10\n",
      "Training loss:....... 0.12218876295405144\n",
      "Validation loss:..... 0.1242513752194366\n",
      "\n",
      "Training accuracy:... 0.9520271021358278\n",
      "Validation accuracy.. 0.9506792357269789\n",
      "\n",
      "EPOCH:  11\n",
      "Training loss:....... 0.1213411702495626\n",
      "Validation loss:..... 0.1216077626453907\n",
      "\n",
      "Training accuracy:... 0.9522964594289609\n",
      "Validation accuracy.. 0.9525826773073409\n",
      "NEW BEST Loss: 0.1216077626453907 ........old best:0.12399640237843548\n",
      "\n",
      "NEW BEST Acc: 0.9525826773073409 ........old best:0.9518854849597581\n",
      "\n",
      "EPOCH:  12\n",
      "Training loss:....... 0.12051412549070613\n",
      "Validation loss:..... 0.11991831730511855\n",
      "\n",
      "Training accuracy:... 0.9525413255752261\n",
      "Validation accuracy.. 0.9531012233309867\n",
      "NEW BEST Loss: 0.11991831730511855 ........old best:0.1216077626453907\n",
      "\n",
      "NEW BEST Acc: 0.9531012233309867 ........old best:0.9525826773073409\n",
      "\n",
      "EPOCH:  13\n",
      "Training loss:....... 0.11972750338564019\n",
      "Validation loss:..... 0.12296327085607381\n",
      "\n",
      "Training accuracy:... 0.9527816968253783\n",
      "Validation accuracy.. 0.9519204470037261\n",
      "\n",
      "EPOCH:  14\n",
      "Training loss:....... 0.11900052938799927\n",
      "Validation loss:..... 0.11761197242070529\n",
      "\n",
      "Training accuracy:... 0.9530100257278179\n",
      "Validation accuracy.. 0.9536416866874766\n",
      "NEW BEST Loss: 0.11761197242070529 ........old best:0.11991831730511855\n",
      "\n",
      "NEW BEST Acc: 0.9536416866874766 ........old best:0.9531012233309867\n",
      "\n",
      "EPOCH:  15\n",
      "Training loss:....... 0.11830875832687862\n",
      "Validation loss:..... 0.11959433520482446\n",
      "\n",
      "Training accuracy:... 0.9532301697641731\n",
      "Validation accuracy.. 0.9530723644764444\n",
      "\n",
      "EPOCH:  16\n",
      "Training loss:....... 0.1176135236436423\n",
      "Validation loss:..... 0.11712913455987217\n",
      "\n",
      "Training accuracy:... 0.9534377587656254\n",
      "Validation accuracy.. 0.9537722936363885\n",
      "NEW BEST Loss: 0.11712913455987217 ........old best:0.11761197242070529\n",
      "\n",
      "NEW BEST Acc: 0.9537722936363885 ........old best:0.9536416866874766\n",
      "\n",
      "EPOCH:  17\n",
      "Training loss:....... 0.11691944042923386\n",
      "Validation loss:..... 0.1183673198827188\n",
      "\n",
      "Training accuracy:... 0.9536661750496868\n",
      "Validation accuracy.. 0.953347327675941\n",
      "\n",
      "EPOCH:  18\n",
      "Training loss:....... 0.11631495406862326\n",
      "Validation loss:..... 0.11665609877819967\n",
      "\n",
      "Training accuracy:... 0.9538388019754297\n",
      "Validation accuracy.. 0.9540555422988148\n",
      "NEW BEST Loss: 0.11665609877819967 ........old best:0.11712913455987217\n",
      "\n",
      "NEW BEST Acc: 0.9540555422988148 ........old best:0.9537722936363885\n",
      "\n",
      "EPOCH:  19\n",
      "Training loss:....... 0.11572174327503779\n",
      "Validation loss:..... 0.11916363961768872\n",
      "\n",
      "Training accuracy:... 0.9540108672657707\n",
      "Validation accuracy.. 0.9528695109003812\n",
      "\n",
      "EPOCH:  20\n",
      "Training loss:....... 0.11507495833716974\n",
      "Validation loss:..... 0.1157519019362501\n",
      "\n",
      "Training accuracy:... 0.9542167996422055\n",
      "Validation accuracy.. 0.9542645675661626\n",
      "NEW BEST Loss: 0.1157519019362501 ........old best:0.11665609877819967\n",
      "\n",
      "NEW BEST Acc: 0.9542645675661626 ........old best:0.9540555422988148\n",
      "\n",
      "EPOCH:  21\n",
      "Training loss:....... 0.1144871545514973\n",
      "Validation loss:..... 0.11515032301848184\n",
      "\n",
      "Training accuracy:... 0.9543923743380056\n",
      "Validation accuracy.. 0.9544591780852089\n",
      "NEW BEST Loss: 0.11515032301848184 ........old best:0.1157519019362501\n",
      "\n",
      "NEW BEST Acc: 0.9544591780852089 ........old best:0.9542645675661626\n",
      "\n",
      "EPOCH:  22\n",
      "Training loss:....... 0.11393245418617379\n",
      "Validation loss:..... 0.11514424093644628\n",
      "\n",
      "Training accuracy:... 0.9545601717069024\n",
      "Validation accuracy.. 0.9546500112160378\n",
      "NEW BEST Loss: 0.11514424093644628 ........old best:0.11515032301848184\n",
      "\n",
      "NEW BEST Acc: 0.9546500112160378 ........old best:0.9544591780852089\n",
      "\n",
      "EPOCH:  23\n",
      "Training loss:....... 0.1134061018300583\n",
      "Validation loss:..... 0.12026867300573021\n",
      "\n",
      "Training accuracy:... 0.9547141189074836\n",
      "Validation accuracy.. 0.9528405346140807\n",
      "\n",
      "EPOCH:  24\n",
      "Training loss:....... 0.11286961895965798\n",
      "Validation loss:..... 0.11456479867199053\n",
      "\n",
      "Training accuracy:... 0.9548771791772668\n",
      "Validation accuracy.. 0.9542730161287563\n",
      "NEW BEST Loss: 0.11456479867199053 ........old best:0.11514424093644628\n",
      "\n",
      "\n",
      "EPOCH:  25\n",
      "Training loss:....... 0.11235402235315142\n",
      "Validation loss:..... 0.11285342695034714\n",
      "\n",
      "Training accuracy:... 0.95502153107812\n",
      "Validation accuracy.. 0.9551863644609954\n",
      "NEW BEST Loss: 0.11285342695034714 ........old best:0.11456479867199053\n",
      "\n",
      "NEW BEST Acc: 0.9551863644609954 ........old best:0.9546500112160378\n",
      "\n",
      "EPOCH:  26\n",
      "Training loss:....... 0.11180942846339441\n",
      "Validation loss:..... 0.11155745497456303\n",
      "\n",
      "Training accuracy:... 0.9551869840089863\n",
      "Validation accuracy.. 0.9555471474416788\n",
      "NEW BEST Loss: 0.11155745497456303 ........old best:0.11285342695034714\n",
      "\n",
      "NEW BEST Acc: 0.9555471474416788 ........old best:0.9551863644609954\n",
      "\n",
      "EPOCH:  27\n",
      "Training loss:....... 0.1113539590991052\n",
      "Validation loss:..... 0.11190885300387438\n",
      "\n",
      "Training accuracy:... 0.9553282043120558\n",
      "Validation accuracy.. 0.955570131446325\n",
      "NEW BEST Acc: 0.955570131446325 ........old best:0.9555471474416788\n",
      "\n",
      "EPOCH:  28\n",
      "Training loss:....... 0.11084913411930561\n",
      "Validation loss:..... 0.11074533326055867\n",
      "\n",
      "Training accuracy:... 0.9554744118939856\n",
      "Validation accuracy.. 0.9558183789208637\n",
      "NEW BEST Loss: 0.11074533326055867 ........old best:0.11155745497456303\n",
      "\n",
      "NEW BEST Acc: 0.9558183789208637 ........old best:0.955570131446325\n",
      "\n",
      "EPOCH:  29\n",
      "Training loss:....... 0.11042562703194911\n",
      "Validation loss:..... 0.10945918877618481\n",
      "\n",
      "Training accuracy:... 0.9555922371275398\n",
      "Validation accuracy.. 0.9561859077036486\n",
      "NEW BEST Loss: 0.10945918877618481 ........old best:0.11074533326055867\n",
      "\n",
      "NEW BEST Acc: 0.9561859077036486 ........old best:0.9558183789208637\n",
      "\n",
      "EPOCH:  30\n"
     ]
    }
   ],
   "source": [
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "minLoss = 99999\n",
    "maxValacc = -99999\n",
    "for epoch in range(1000):\n",
    "    scheduler.step()\n",
    "    print('EPOCH: ',epoch+1)\n",
    "#     train_losses = []\n",
    "#     val_losses = []    \n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    count = 0\n",
    "    for images, labels in train_loader:    \n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        train_acc.append(accuracy(outputs, labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count +=1\n",
    "    \n",
    "    print('Training loss:.......', running_loss/count)\n",
    "#     print('Training accuracy:...', np.mean(train_acc))\n",
    "    mean_train_losses.append(running_loss/count)\n",
    "        \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    val_running_loss = 0.0\n",
    "    for images, labels in val_loader:\n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "                \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_acc.append(accuracy(outputs, labels))\n",
    "        val_running_loss += loss.item()\n",
    "        count +=1\n",
    "\n",
    "    mean_val_loss = val_running_loss/count\n",
    "    print('Validation loss:.....', mean_val_loss)\n",
    "    print('')    \n",
    "    print('Training accuracy:...', np.mean(train_acc))\n",
    "    print('Validation accuracy..', np.mean(val_acc))\n",
    "    \n",
    "    mean_val_losses.append(mean_val_loss)\n",
    "    \n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    \n",
    "    val_acc_ = np.mean(val_acc)\n",
    "    mean_val_acc.append(val_acc_)\n",
    "    \n",
    "   \n",
    "    if mean_val_loss < minLoss:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/att_res_torch_190k.pth' )\n",
    "        print(f'NEW BEST Loss: {mean_val_loss} ........old best:{minLoss}')\n",
    "        minLoss = mean_val_loss\n",
    "        print('')\n",
    "    \n",
    "    if (epoch+1)%100 ==0:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/att_res_torch_190k_'+str(epoch+1)+'.pth')\n",
    "        \n",
    "        \n",
    "    if val_acc_ > maxValacc:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/best_acc.pth' )\n",
    "        print(f'NEW BEST Acc: {val_acc_} ........old best:{maxValacc}')\n",
    "        maxValacc = val_acc_\n",
    "    \n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCH:  53\n",
    "Training loss:....... 0.09136750614047497\n",
    "Validation loss:..... 0.10392922850369604\n",
    "NEW BEST Loss: 0.10392922850369604 ........old best:0.10768691763934062\n",
    "\n",
    "\n",
    "EPOCH:  54\n",
    "Training loss:....... 0.09125455257533018\n",
    "Validation loss:..... 0.12576120675412894\n",
    "\n",
    "EPOCH:  55\n",
    "Training loss:....... 0.09110646387617924\n",
    "Validation loss:..... 0.12712479337598337\n",
    "\n",
    "EPOCH:  56\n",
    "Training loss:....... 0.09107536044317538\n",
    "Validation loss:..... 0.14051407174229222\n",
    "\n",
    "EPOCH:  57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11\n"
     ]
    }
   ],
   "source": [
    "print(str(round(1.1111,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
