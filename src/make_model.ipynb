{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms as tf\n",
    "\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "# from sklearn import preprocessing\n",
    "from skimage import transform\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from imgaug import augmenters as iaaot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../networks/')\n",
    "from Att_Net import Att_Net\n",
    "# from help_functions import *\n",
    "\n",
    "# #function to obtain data for training/testing (validation)\n",
    "# from extract_patches import get_data_training\n",
    "# sys.path.insert(0, '../lib/networks/')\n",
    "\n",
    "from preprocessing import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train images/masks shape:\n",
      "(20, 1, 565, 565)\n",
      "train images range (min-max): 0.0 - 1.0\n",
      "train masks are within 0-1\n",
      "\n",
      "patches per full image: 9500\n"
     ]
    }
   ],
   "source": [
    "train_img, label_img = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img.shape, label_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_subimgs = 190000\n",
    "indices = list(range(N_subimgs))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_size = 1/10\n",
    "split = np.int_(np.floor(val_size * N_subimgs))\n",
    "\n",
    "train_idxs = indices[split:]\n",
    "val_idxs = indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,preprocessed_images, train=True, label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.images = preprocessed_images\n",
    "        if self.train:\n",
    "            self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        img = np.zeros_like(image, dtype=np.float32)\n",
    "        \n",
    "        img += image\n",
    "        label = None\n",
    "        if self.train:\n",
    "            label = self.label[idx]\n",
    "#             msk = np.zeros((2,48,48), dtype=np.long)\n",
    "#             msk[1] = label\n",
    "#             msk[0] = 1-label\n",
    "            \n",
    "#             msk += label\n",
    "            return (img, label)\n",
    "        return img\n",
    "\n",
    "eye_dataset_train = eye_dataset(train_img[train_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[train_idxs])\n",
    "\n",
    "eye_dataset_val = eye_dataset(train_img[val_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[val_idxs])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=eye_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=eye_dataset_val, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Att_Net(32)\n",
    "model.cuda()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "                             weight_decay=.0001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, \n",
    "                                                 milestones=[400,600,700], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 0.0001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "#                              weight_decay=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    total = 0.0\n",
    "    predicted = torch.exp(out)\n",
    "    size = predicted.shape[0]*predicted.shape[2]\n",
    "    pred = torch.argmax(predicted.data, dim=1)\n",
    "    total += torch.sum(pred == labels.data)\n",
    "    return total.cpu().detach().numpy()/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "minLoss = 99999\n",
    "maxValacc = -99999\n",
    "for epoch in range(1000):\n",
    "    scheduler.step()\n",
    "    print('EPOCH: ',epoch+1)\n",
    "#     train_losses = []\n",
    "#     val_losses = []    \n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    count = 0\n",
    "    for images, labels in train_loader:    \n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images) \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        train_acc.append(accuracy(outputs, labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count +=1\n",
    "    \n",
    "    print('Training loss:.......', running_loss/count)\n",
    "#     print('Training accuracy:...', np.mean(train_acc))\n",
    "    mean_train_losses.append(running_loss/count)\n",
    "        \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    val_running_loss = 0.0\n",
    "    for images, labels in val_loader:\n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "                \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        val_acc.append(accuracy(outputs, labels))\n",
    "        val_running_loss += loss.item()\n",
    "        count +=1\n",
    "\n",
    "    mean_val_loss = val_running_loss/count\n",
    "    print('Validation loss:.....', mean_val_loss)\n",
    "    print('')    \n",
    "    print('Training accuracy:...', np.mean(train_acc))\n",
    "    print('Validation accuracy..', np.mean(val_acc))\n",
    "    \n",
    "    mean_val_losses.append(mean_val_loss)\n",
    "    \n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    \n",
    "    val_acc_ = np.mean(val_acc)\n",
    "    mean_val_acc.append(val_acc_)\n",
    "    \n",
    "   \n",
    "    if mean_val_loss < minLoss:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/att_res_torch_190k_.pth' )\n",
    "        print(f'NEW BEST Loss: {mean_val_loss} ........old best:{minLoss}')\n",
    "        minLoss = mean_val_loss\n",
    "        print('')\n",
    "    \n",
    "    if (epoch+1)%100 ==0:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/att_res_torch_190k_'+str(epoch+1)+'.pth')\n",
    "        \n",
    "        \n",
    "    if val_acc_ > maxValacc:\n",
    "        torch.save(model.state_dict(), 'expr/att_bott/best_acc.pth' )\n",
    "        print(f'NEW BEST Acc: {val_acc_} ........old best:{maxValacc}')\n",
    "        maxValacc = val_acc_\n",
    "    \n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCH:  53\n",
    "Training loss:....... 0.09136750614047497\n",
    "Validation loss:..... 0.10392922850369604\n",
    "NEW BEST Loss: 0.10392922850369604 ........old best:0.10768691763934062\n",
    "\n",
    "\n",
    "EPOCH:  54\n",
    "Training loss:....... 0.09125455257533018\n",
    "Validation loss:..... 0.12576120675412894\n",
    "\n",
    "EPOCH:  55\n",
    "Training loss:....... 0.09110646387617924\n",
    "Validation loss:..... 0.12712479337598337\n",
    "\n",
    "EPOCH:  56\n",
    "Training loss:....... 0.09107536044317538\n",
    "Validation loss:..... 0.14051407174229222\n",
    "\n",
    "EPOCH:  57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11\n"
     ]
    }
   ],
   "source": [
    "print(str(round(1.1111,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
