{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms as tf\n",
    "\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "from sklearn import preprocessing\n",
    "from skimage import transform\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from imgaug import augmenters as iaaot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,preprocessed_images, train=True, label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.images = preprocessed_images\n",
    "        if self.train:\n",
    "            self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        img = np.zeros_like(image, dtype=np.float32)\n",
    "        \n",
    "        img += image\n",
    "        label = None\n",
    "        if self.train:\n",
    "            label = self.label[idx]\n",
    "#             msk = np.zeros((2,48,48), dtype=np.long)\n",
    "#             msk[1] = label\n",
    "#             msk[0] = 1-label\n",
    "            \n",
    "#             msk += label\n",
    "            return (img, label)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sys\n",
    "sys.path.insert(0, '../lib/')\n",
    "# help_functions.py\n",
    "from help_functions import * \n",
    "\n",
    "# sys.path.insert(0, '../networks/')\n",
    "# from multihead_attention import multihead_attention_2d\n",
    "# from attention_model import unet_bot_att\n",
    "# extract_patches.py\n",
    "from extract_patches import recompone\n",
    "from extract_patches import recompone_overlap\n",
    "from extract_patches import paint_border\n",
    "from extract_patches import kill_border\n",
    "from extract_patches import pred_only_FOV\n",
    "from extract_patches import get_data_testing\n",
    "from extract_patches import get_data_testing_overlap\n",
    "# pre_processing.py\n",
    "from pre_processing import my_PreProc\n",
    "# from pixel_dcn import pixel_dcl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [data paths]\n",
    "path_local =  '../DRIVE_datasets_training_testing/'\n",
    "train_imgs_original = 'DRIVE_dataset_imgs_train.hdf5'\n",
    "train_groundTruth = 'DRIVE_dataset_groundTruth_train.hdf5'\n",
    "train_border_masks = 'DRIVE_dataset_borderMasks_train.hdf5'\n",
    "test_imgs_original = 'DRIVE_dataset_imgs_test.hdf5'\n",
    "test_groundTruth = 'DRIVE_dataset_groundTruth_test.hdf5'\n",
    "test_border_masks = 'DRIVE_dataset_borderMasks_test.hdf5'\n",
    "\n",
    "\n",
    "\n",
    "#========= CONFIG FILE TO READ FROM =======\n",
    "# config = configparser.RawConfigParser()\n",
    "# config.read('../configuration.txt')\n",
    "# #===========================================\n",
    "# #run the training on invariant or local\n",
    "# path_data = config.get('data paths', 'path_local')\n",
    "\n",
    "#original test images (for FOV selection)\n",
    "DRIVE_test_imgs_original = path_local + test_imgs_original\n",
    "\n",
    "test_imgs_orig = load_hdf5(DRIVE_test_imgs_original)\n",
    "full_img_height = test_imgs_orig.shape[2]\n",
    "full_img_width = test_imgs_orig.shape[3]\n",
    "#the border masks provided by the DRIVE\n",
    "DRIVE_test_border_masks = path_local + test_border_masks\n",
    "test_border_masks = load_hdf5(DRIVE_test_border_masks)\n",
    "# dimension of the patches\n",
    "patch_height = 48\n",
    "patch_width = 48\n",
    "#the stride in case output with average\n",
    "stride_height = 5\n",
    "stride_width = 5\n",
    "assert (stride_height < patch_height and stride_width < patch_width)\n",
    "#model name\n",
    "name_experiment = 'test'\n",
    "path_experiment = '../' +name_experiment +'/'\n",
    "#N full images to be predicted\n",
    "Imgs_to_test = 20\n",
    "#Grouping of the predicted images\n",
    "N_visual = 1\n",
    "#====== average mode ===========\n",
    "average_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the side H is not compatible with the selected stride of 5\n",
      "img_h 584, patch_h 48, stride_h 5\n",
      "(img_h - patch_h) MOD stride_h: 1\n",
      "So the H dim will be padded with additional 4 pixels\n",
      "the side W is not compatible with the selected stride of 5\n",
      "img_w 565, patch_w 48, stride_w 5\n",
      "(img_w - patch_w) MOD stride_w: 2\n",
      "So the W dim will be padded with additional 3 pixels\n",
      "new full images shape: \n",
      "(20, 1, 588, 568)\n",
      "\n",
      "test images shape:\n",
      "(20, 1, 588, 568)\n",
      "\n",
      "test mask shape:\n",
      "(20, 1, 584, 565)\n",
      "test images range (min-max): 0.0 - 1.0\n",
      "test masks are within 0-1\n",
      "\n",
      "Number of patches on h : 109\n",
      "Number of patches on w : 105\n",
      "number of patches per image: 11445, totally for this dataset: 228900\n",
      "\n",
      "test PATCHES images shape:\n",
      "(228900, 1, 48, 48)\n",
      "test PATCHES images range (min-max): 0.0 - 1.0\n"
     ]
    }
   ],
   "source": [
    "patches_imgs_test = None\n",
    "new_height = None\n",
    "new_width = None\n",
    "masks_test  = None\n",
    "patches_masks_test = None\n",
    "if average_mode == True:\n",
    "    patches_imgs_test, new_height, new_width, masks_test = get_data_testing_overlap(\n",
    "        DRIVE_test_imgs_original = DRIVE_test_imgs_original,  #original\n",
    "        DRIVE_test_groudTruth = path_local + test_groundTruth, #masks\n",
    "        Imgs_to_test = Imgs_to_test,\n",
    "        patch_height = patch_height,\n",
    "        patch_width = patch_width,\n",
    "        stride_height = stride_height,\n",
    "        stride_width = stride_width\n",
    "    )\n",
    "    #masks_test = np.rollaxis(masks_test, 1, 2)\n",
    "else:\n",
    "    patches_imgs_test, patches_masks_test = get_data_testing(\n",
    "        DRIVE_test_imgs_original = DRIVE_test_imgs_original,  #original\n",
    "        DRIVE_test_groudTruth = path_local +test_groundTruth,  #masks\n",
    "        Imgs_to_test = Imgs_to_test,\n",
    "        patch_height = patch_height,\n",
    "        patch_width = patch_width,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228900, 1, 48, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_imgs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_dataset_test = eye_dataset(patches_imgs_test, \n",
    "                                      train=False, \n",
    "                                      label=None)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=eye_dataset_test, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(chanIn,filters, qkv,layer_type='SAME'):\n",
    "    '''\n",
    "    Args:\n",
    "        inputs: a Tensor with shape [batch,channels, h, w]\n",
    "        total_key_filters: an integer\n",
    "        total_value_filters: and integer\n",
    "        layer_type: String, type of this layer -- SAME, DOWN, UP\n",
    "\n",
    "    Returns:\n",
    "        q: [batch, _h, _w, total_key_filters] tensor\n",
    "        k: [batch, h, w, total_key_filters] tensor\n",
    "\t\tv: [batch, h, w, total_value_filters] tensor\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "#     print('compute_qkv .........')\n",
    "    if qkv == 'q':\n",
    "        # linear transformation for q , filters = key_filters\n",
    "        if layer_type == 'SAME':\n",
    "            qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True,padding=0)\n",
    "        elif layer_type == 'DOWN':\n",
    "            qkv = nn.Conv2d(chanIn, filters, 3, 2, bias=True, padding =1)\n",
    "        elif layer_type == 'UP':\n",
    "            qkv = nn.ConvTranspose2d(chanIn, filtersx, 3, 2, bias=True, padding=1)\n",
    "    \n",
    "    if qkv == 'k':\n",
    "        # linear transformation for k\n",
    "        qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True, padding=0)\n",
    "\n",
    "    if qkv =='v':\n",
    "        # linear transformation for k, value filtesr\n",
    "        qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True, padding=0)\n",
    "\n",
    "    return qkv\n",
    "\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"Split channels (last dimension) into multiple heads (becomes dimension 1).\n",
    "\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, h, w, channels]\n",
    "        num_heads: an integer\n",
    "\n",
    "    Returns:\n",
    "        a Tensor with shape [batch, num_heads, h, w, channels / num_heads]\n",
    "    \"\"\"\n",
    "    \n",
    "    return split_last_dimension(x, num_heads).permute(0,4,1,2,3)#permute(0,3,1,2,4)\n",
    "\n",
    "def split_last_dimension(x,n):\n",
    "    \"\"\"Reshape x so that the last dimension becomes two dimensions.\n",
    "    The first of these two dimensions is n.\n",
    "    Args:\n",
    "        x: a Tensor with shape [..., m]\n",
    "        n: an integer.\n",
    "    Returns:\n",
    "        a Tensor with shape [..., n, m/n]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    chunk_size = int(x.shape[3]/ n)\n",
    "    ret = torch.unsqueeze(x,4)\n",
    "    \n",
    "    ret = torch.cat(ret.split(split_size=chunk_size, dim=3),4)#.permute(0,1,2,4,3)\n",
    "#     print('split', ret.shape)\n",
    "#     ret.view(new_shape)\n",
    "#     print('split view ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "def global_attention(q, k, v, chan_z):\n",
    "    \"\"\"global self-attention.\n",
    "    Args:\n",
    "        q: a Tensor with shape [batch, heads, _d, _h, _w, channels_k]\n",
    "        k: a Tensor with shape [batch, heads, d, h, w, channels_k]\n",
    "        v: a Tensor with shape [batch, heads, d, h, w, channels_v]\n",
    "        name: an optional string\n",
    "    Returns:\n",
    "        a Tensor of shape [batch, heads, _d, _h, _w, channels_v]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     print(new_shape)\n",
    "    # flatten q,k,v\n",
    "    q_new = flatten(q)\n",
    "    k_new = flatten(k)\n",
    "    v_new = flatten(v)\n",
    "\n",
    "    # attention\n",
    "    output = dot_product_attention(q_new, k_new, v_new, bias=None,\n",
    "                dropout_rate=0.5)\n",
    "#     print('outp ', output.shape)\n",
    "\n",
    "    # putting the representations back in the right place\n",
    "#     print('output before scatter', output.shape)\n",
    "    output = scatter(output, chan_z)\n",
    "\n",
    "    return output\n",
    "\n",
    "def dot_product_attention(q, k, v, bias,dropout_rate=0.0):\n",
    "    \"\"\"Dot-product attention.\n",
    "    Args:\n",
    "        q: a Tensor with shape [batch, heads, length_q, channels_k]\n",
    "        k: a Tensor with shape [batch, heads, length_kv, channels_k]\n",
    "        v: a Tensor with shape [batch, heads, length_kv, channels_v]\n",
    "        bias: bias Tensor\n",
    "        dropout_rate: a floating point number\n",
    "        name: an optional string\n",
    "    Returns:\n",
    "        A Tensor with shape [batch, heads, length_q, channels_v]\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # [batch, num_heads, length_q, length_kv]\n",
    "#     print(q.shape, k.transpose(2,3).shape)\n",
    "    logits = torch.matmul(q, k.transpose(2,3))\n",
    "\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "\n",
    "    weights = F.softmax(logits)\n",
    "\n",
    "    # dropping out the attention links for each of the heads\n",
    "    weights = F.dropout(weights, dropout_rate)\n",
    "\n",
    "    return torch.matmul(weights, v)\n",
    "\n",
    "\n",
    "def reshape_range(tensor, i, j, shape):\n",
    "    \"\"\"Reshapes a tensor between dimensions i and j.\"\"\"\n",
    "\n",
    "    target_shape = tf.concat(\n",
    "            [tf.shape(tensor)[:i], shape, tf.shape(tensor)[j:]],\n",
    "            axis=0)\n",
    "\n",
    "    return tf.reshape(tensor, target_shape)\n",
    "\n",
    "\n",
    "\n",
    "def scatter(x, chn):\n",
    "    \"\"\"scatter x.\"\"\"\n",
    "    \n",
    "#     print('scatter shape in ', x.shape)\n",
    "\n",
    "    x = x.view(x.shape[0],chn,chn,x.shape[3],-1)\n",
    "#     print('scatter ', x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"flatten x.\"\"\"\n",
    "    # [batch, heads, h,w, ch/h]\n",
    "    # [batch, heads, length, channels], length = d*h*w\n",
    "    \n",
    "    l  = x.shape[2] * x.shape[3]\n",
    "    # [batch, heads, length, channels], length = d*h*w\n",
    "    x = x.view(x.shape[0], x.shape[1], l,-1)\n",
    "#     print('flatten shape', x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def combine_heads(x):\n",
    "    \"\"\"Inverse of split_heads_3d.\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, num_heads, d, h, w, channels / num_heads]\n",
    "    Returns:\n",
    "        a Tensor with shape [batch, d, h, w, channels]\n",
    "    \"\"\"\n",
    "#  [0, 2, 3, 4, 1, 5]\n",
    "#     print('combine heads ,', x.shape)\n",
    "    return combine_last_two_dimensions(x)\n",
    "\n",
    "\n",
    "def combine_last_two_dimensions(x):\n",
    "    \"\"\"Reshape x so that the last two dimension become one.\n",
    "    Args:\n",
    "        x: a Tensor with shape [..., a, b]\n",
    "    Returns:\n",
    "        a Tensor with shape [..., a*b]\n",
    "    \"\"\"\n",
    "\n",
    "#     old_shape = x.get_shape().dims\n",
    "#     a, b = old_shape[-2:]\n",
    "#     new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "\n",
    "#     ret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n",
    "#     ret.set_shape(new_shape)\n",
    "    \n",
    "    x = x.contiguous().view(x.shape[0],x.shape[1], x.shape[2], -1)\n",
    "#     print('combine last two ', x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"This script defines 3D different multi-head attention layers.\n",
    "(12,12,12,.5,2,False)\n",
    "\"\"\"\n",
    "class MultiHeadAttention_(nn.Module):\n",
    "    def __init__(self,\n",
    "                 chanIn,\n",
    "                 output_filters,\n",
    "                 total_key_filters,\n",
    "                 total_value_filters,                 \n",
    "                 num_heads,\n",
    "                 layer_type='SAME'):\n",
    "        super(MultiHeadAttention_, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        inputs: channels first as input [batch, chn, h,w]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \"\"\"3d Multihead scaled-dot-product attention with input/output transformations.\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor with shape [batch, h, w, channels]\n",
    "            \n",
    "            total_key_filters: an integer. Note that queries have the same number \n",
    "                of channels as keys\n",
    "            total_value_filters: an integer\n",
    "            output_depth: an integer\n",
    "            num_heads: an integer dividing total_key_filters and total_value_filters\n",
    "            layer_type: a string, type of this layer -- SAME, DOWN, UP\n",
    "            name: an optional string\n",
    "        Returns:\n",
    "            A Tensor of shape [batch, _d, _h, _w, output_filters]\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if the total_key_filters or total_value_filters are not divisible\n",
    "                by the number of attention heads.\n",
    "        \"\"\"\n",
    "\n",
    "        if total_key_filters % num_heads != 0:\n",
    "            raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n",
    "                            \"attention heads (%d).\" % (total_key_filters, num_heads))\n",
    "        if total_value_filters % num_heads != 0:\n",
    "            raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n",
    "                            \"attention heads (%d).\" % (total_value_filters, num_heads))\n",
    "        if layer_type not in ['SAME', 'DOWN', 'UP']:\n",
    "            raise ValueError(\"Layer type (%s) must be one of SAME, \"\n",
    "                            \"DOWN, UP.\" % (layer_type))\n",
    "            \n",
    "        \n",
    "        '''\n",
    "        inputs: [batch, chn, h,w]\n",
    "        output: [batch, chn, h,w]\n",
    "        next step, permute to [batch, h,w, chn]\n",
    "        '''\n",
    "        self.q = compute_qkv(chanIn, total_key_filters,'q',layer_type='SAME')\n",
    "        self.k = compute_qkv(chanIn, total_key_filters,'k',layer_type='SAME')\n",
    "        self.v = compute_qkv(chanIn, total_value_filters,'v',layer_type='SAME')\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.total_key_filters = total_key_filters\n",
    "        self.total_value_filters = total_value_filters\n",
    "        \n",
    "        self.conv = nn.Conv2d(total_key_filters, output_filters, 1,1,bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "#         print('x before q shape in: ', x.shape)\n",
    "        #[batch, chn, h,w]\n",
    "        q = self.q(x)\n",
    "#         print('q shape in: ', q.shape)\n",
    "        k = self.k(x)\n",
    "#         print('k shape in: ', k.shape)\n",
    "        v = self.v(x)\n",
    "#         print('v shape in: ', v.shape)\n",
    "        \n",
    "        #permute to set [batch, h,w, chn]\n",
    "        q = q.permute(0,2,3,1)\n",
    "#         print('q after permute ', q.shape)\n",
    "        k = k.permute(0,2,3,1)\n",
    "        v = v.permute(0,2,3,1)\n",
    "        \n",
    "#         print('q shape before split ' ,q.shape[3], q.shape)\n",
    "        q = split_heads(q,self.num_heads)    \n",
    "#         print('q shape after split ' ,q.shape[3], q.shape)\n",
    "        k = split_heads(k,self.num_heads)\n",
    "        v = split_heads(v,self.num_heads)\n",
    "        #after split [batch, heads, h,w,k/h]\n",
    "#         print('q split ', q.shape)\n",
    "\n",
    "        #normalize\n",
    "        key_filters_per_head = self.total_key_filters // self.num_heads\n",
    "        q *= key_filters_per_head**-0.5\n",
    "        \n",
    "        att = global_attention(q,k,v, q.shape[2])\n",
    "#         print('out of attt : ', att.shape)\n",
    "\n",
    "\n",
    "\n",
    "        x = combine_heads(att)\n",
    "        \n",
    "#         print('LAST shape in, ' ,x.shape)\n",
    "        x = x.permute(0,3,1,2)\n",
    "#         print('LAST shape in, ' ,x.shape)\n",
    "        x = self.conv(x)\n",
    "#         print('out of att', x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "def bn_relu(chanIn, chanOut, ks = 3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(chanIn),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Conv2d(chanIn, chanOut, ks, stride, padding=1),\n",
    "        \n",
    "    )\n",
    "\n",
    "class Att_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_block_1 = bn_relu(1, 1, 3,1)\n",
    "        self.input_block_2 = bn_relu(1, 1, 3,1)\n",
    "        self.input_1x1 = nn.Conv2d(1,32,1,1)\n",
    "        \n",
    "        self.down1_block_1 = bn_relu(32, 64, 3,2)\n",
    "        self.down1_block_2 = bn_relu(64,64, 3,1)\n",
    "        self.down1_1x1 = nn.Conv2d(32, 64, 1,2)\n",
    "        \n",
    "        self.down2_block_1 = bn_relu(64, 128, 3,2)\n",
    "        self.down2_block_2 = bn_relu(128, 128, 3,1)\n",
    "        self.down2_1x1 = nn.Conv2d(64, 128, 1,2)\n",
    "        \n",
    "        self.mid = MultiHeadAttention_(128,128,32,32,4)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "#         self.mid = nn.Conv2d(128,128,3,padding=1)\n",
    "#         self.bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2)\n",
    "        self.up2_1 = nn.Conv2d(128,64,3,1,1)\n",
    "        \n",
    "        self.up1 = nn.Upsample(scale_factor=2)\n",
    "        self.up1_1 = nn.Conv2d(64,32,3,1,1)\n",
    "#         self.up1m = MultiHeadAttention_(32,32,12,12,4)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.out = nn.Conv2d(32,1,3,1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('input shape ', x.shape)\n",
    "        x_top = x\n",
    "        x = self.input_block_1(x)\n",
    "        x = self.input_block_2(x)\n",
    "        x = torch.add(x_top,x) #res1\n",
    "#         print('x before 1x1 shape: ',x.shape)\n",
    "        x_res1 = self.input_1x1(x)\n",
    "#         print('xres1  ', x_res1.shape)\n",
    "        \n",
    "        x_l = self.down1_1x1(x_res1)\n",
    "        x = self.down1_block_1(x_res1)\n",
    "        x = self.down1_block_2(x)\n",
    "        x_res2 = torch.add(x_l, x) \n",
    "        x_l = self.down2_1x1(x_res2)\n",
    "        x = self.down2_block_1(x_res2)\n",
    "        x = self.down2_block_2(x)\n",
    "        x_res3 = torch.add(x_l,x) \n",
    "        \n",
    "#       res3 shape  torch.Size([32, 128, 12, 12])\n",
    "        x = self.mid(x_res3)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.up2_1(x)\n",
    "        \n",
    "        x = torch.add(x, x_res2)\n",
    "        \n",
    "        x = self.up1(x)\n",
    "        x = self.up1_1(x)\n",
    "        x = torch.add(x, x_res1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "#         x = x.view(x.shape[0], x.shape[1],-1)\n",
    "#         x = x.permute(0,2,1)\n",
    "# #         print('out : ', x.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "# model = Att_Net()\n",
    "# model.cuda()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# learning_rate = 0.001\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "#                              weight_decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Att_Net(\n",
       "  (input_block_1): Sequential(\n",
       "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (input_block_2): Sequential(\n",
       "    (0): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (input_1x1): Conv2d(1, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (down1_block_1): Sequential(\n",
       "    (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (down1_block_2): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (down1_1x1): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (down2_block_1): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (down2_block_2): Sequential(\n",
       "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU6(inplace)\n",
       "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (down2_1x1): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "  (mid): MultiHeadAttention_(\n",
       "    (q): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (k): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (v): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (up2): Upsample(scale_factor=2, mode=nearest)\n",
       "  (up2_1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (up1): Upsample(scale_factor=2, mode=nearest)\n",
       "  (up1_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Att_Net()\n",
    "model.cuda()\n",
    "state_dict = torch.load('att_res_torch_190k_222.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228900, 1, 48, 48)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_images = np.zeros((len(patches_imgs_test),2304,2)) \n",
    "pred_images=patches_imgs_test\n",
    "pred_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:122: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "count =0\n",
    "for image in test_loader:\n",
    "    image = Variable(image.cuda())\n",
    "    outputs = model(image)\n",
    "#     print(outputs.shape)\n",
    "#     print(outputs)\n",
    "    for i in range(outputs.shape[0]):\n",
    "#         print(outputs[i].shape)\n",
    "#         print(F.sigmoid(outputs)[i].data.cpu().numpy())\n",
    "        \n",
    "        pred_images[count,:,:] =F.sigmoid(outputs)[i].data.cpu().numpy()\n",
    "#         print('4444444444444444444444444444444444',pred_images[count])\n",
    "        count +=1\n",
    "#         print(F.sigmoid(outputs)[i][0].data.cpu().numpy())\n",
    "#         print(F.sigmoid(outputs)[i][0].data.cpu().numpy())\n",
    "#     print(outputs.shape)\n",
    "# #     for i in range(16):\n",
    "# #     print(outputs.shape)\n",
    "#     image_gray(F.sigmoid(outputs)[count][1].data.cpu().numpy(),3)\n",
    "#     image_gray(get_it[0].squeeze()[count,:,:],3)\n",
    "#     count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228900, 1, 48, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_reshape = pred_images[0]\n",
    "# test_reshape = x.view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_patches = pred_to_imgs(pred_images, patch_height, patch_width, \"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_patches = pred_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_gray(img, sz=10, color='gray'):\n",
    "    plt.figure()\n",
    "    plt.subplots(figsize=(sz,sz))\n",
    "    plt.imshow(img, cmap=color)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_gray(pred_patches[1000].squeeze(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01925177e-03, 1.95021039e-05, 3.24262260e-06, ...,\n",
       "        4.22906805e-06, 4.66925158e-05, 3.23761394e-03],\n",
       "       [2.37868098e-05, 7.06453562e-08, 8.04361200e-09, ...,\n",
       "        1.93062597e-08, 5.24762015e-07, 2.26488148e-04],\n",
       "       [6.02677301e-06, 1.65949494e-08, 4.56859706e-09, ...,\n",
       "        3.12801802e-08, 6.09542042e-07, 2.22727031e-04],\n",
       "       ...,\n",
       "       [1.39939308e-01, 4.30292413e-02, 1.68317463e-02, ...,\n",
       "        6.73497915e-02, 1.11723118e-01, 2.09269032e-01],\n",
       "       [1.98320493e-01, 9.03301239e-02, 4.59838845e-02, ...,\n",
       "        8.37789699e-02, 1.18127689e-01, 2.08072260e-01],\n",
       "       [2.79093385e-01, 1.88147411e-01, 1.35094598e-01, ...,\n",
       "        1.55468836e-01, 1.87361106e-01, 2.70902127e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_patches[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_patches_h: 109\n",
      "N_patches_w: 105\n",
      "N_patches_img: 11445\n",
      "According to the dimension inserted, there are 20 full images (of 588x568 each)\n",
      "(20, 1, 588, 568)\n",
      "Orig imgs shape: (20, 1, 584, 565)\n",
      "pred imgs shape: (20, 1, 584, 565)\n",
      "Gtruth imgs shape: (20, 1, 584, 565)\n",
      "\n",
      "\n",
      "========  Evaluate the results =======================\n",
      "Calculating results only inside the FOV:\n",
      "y scores pixels: 4538143 (radius 270: 270*270*3.14==228906), including background around retina: 6599200 (584*565==329960)\n",
      "y true pixels: 4538143 (radius 270: 270*270*3.14==228906), including background around retina: 6599200 (584*565==329960)\n",
      "\n",
      "Area under the ROC curve: 0.9490648347853536\n",
      "\n",
      "Area under Precision-Recall curve: 0.7701117143899047\n",
      "\n",
      "Confusion matrix:  Custom threshold (for positive) of 0.5\n",
      "[[3858790  101704]\n",
      " [ 205225  372424]]\n",
      "Global Accuracy: 0.9323668293396661\n",
      "Specificity: 0.9743203751855198\n",
      "Sensitivity: 0.6447236989936795\n",
      "Precision: 0.7854925252252556\n",
      "\n",
      "Jaccard similarity score: 0.9323668293396661\n",
      "\n",
      "F1 score (F-measure): 0.7081805363684507\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_imgs = None\n",
    "orig_imgs = None\n",
    "gtruth_masks = None\n",
    "if average_mode == True:\n",
    "    pred_imgs = recompone_overlap(pred_patches, new_height, new_width, stride_height, stride_width)# predictions\n",
    "    orig_imgs = my_PreProc(test_imgs_orig[0:pred_imgs.shape[0],:,:,:])    #originals\n",
    "    gtruth_masks = masks_test  #ground truth masks\n",
    "else:\n",
    "    pred_imgs = recompone(pred_patches,13,12)       # predictions\n",
    "    orig_imgs = recompone(patches_imgs_test,13,12)  # originals\n",
    "    gtruth_masks = recompone(patches_masks_test,13,12)  #masks\n",
    "# apply the DRIVE masks on the repdictions #set everything outside the FOV to zero!!\n",
    "kill_border(pred_imgs, test_border_masks)  #DRIVE MASK  #only for visualization\n",
    "## back to original dimensions\n",
    "orig_imgs = orig_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "pred_imgs = pred_imgs[:,:,0:full_img_height,0:full_img_width]\n",
    "gtruth_masks = gtruth_masks[:,:,0:full_img_height,0:full_img_width]\n",
    "print(\"Orig imgs shape: \" +str(orig_imgs.shape))\n",
    "print(\"pred imgs shape: \" +str(pred_imgs.shape))\n",
    "print(\"Gtruth imgs shape: \" +str(gtruth_masks.shape))\n",
    "visualize(group_images(orig_imgs,N_visual),path_experiment+\"all_originals\")#.show()\n",
    "visualize(group_images(pred_imgs,N_visual),path_experiment+\"all_predictions\")#.show()\n",
    "visualize(group_images(gtruth_masks,N_visual),path_experiment+\"all_groundTruths\")#.show()\n",
    "#visualize results comparing mask and prediction:\n",
    "assert (orig_imgs.shape[0]==pred_imgs.shape[0] and orig_imgs.shape[0]==gtruth_masks.shape[0])\n",
    "N_predicted = orig_imgs.shape[0]\n",
    "group = N_visual\n",
    "assert (N_predicted%group==0)\n",
    "for i in range(int(N_predicted/group)):\n",
    "    orig_stripe = group_images(orig_imgs[i*group:(i*group)+group,:,:,:],group)\n",
    "    masks_stripe = group_images(gtruth_masks[i*group:(i*group)+group,:,:,:],group)\n",
    "    pred_stripe = group_images(pred_imgs[i*group:(i*group)+group,:,:,:],group)\n",
    "    total_img = np.concatenate((orig_stripe,masks_stripe,pred_stripe),axis=0)\n",
    "    visualize(total_img,path_experiment+name_experiment +\"_Original_GroundTruth_Prediction\"+str(i))#.show()\n",
    "\n",
    "\n",
    "#====== Evaluate the results\n",
    "print(\"\\n\\n========  Evaluate the results =======================\")\n",
    "#predictions only inside the FOV\n",
    "y_scores, y_true = pred_only_FOV(pred_imgs,gtruth_masks, test_border_masks)  #returns data only inside the FOV\n",
    "print(\"Calculating results only inside the FOV:\")\n",
    "print(\"y scores pixels: \" +str(y_scores.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(pred_imgs.shape[0]*pred_imgs.shape[2]*pred_imgs.shape[3]) +\" (584*565==329960)\")\n",
    "print(\"y true pixels: \" +str(y_true.shape[0]) +\" (radius 270: 270*270*3.14==228906), including background around retina: \" +str(gtruth_masks.shape[2]*gtruth_masks.shape[3]*gtruth_masks.shape[0])+\" (584*565==329960)\")\n",
    "\n",
    "#Area under the ROC curve\n",
    "AUC_ROC = roc_auc_score(y_true, y_scores)\n",
    "# fpr, tpr, thresholds = roc_curve((y_true), y_scores)\n",
    "\n",
    "# test_integral = np.trapz(tpr,fpr) #trapz is numpy integration\n",
    "print(\"\\nArea under the ROC curve: \" +str(AUC_ROC))\n",
    "# roc_curve =plt.figure()\n",
    "# plt.plot(fpr,tpr,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_ROC)\n",
    "# plt.title('ROC curve')\n",
    "# plt.xlabel(\"FPR (False Positive Rate)\")\n",
    "# plt.ylabel(\"TPR (True Positive Rate)\")\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.savefig(path_experiment+\"ROC.png\")\n",
    "\n",
    "#Precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "precision = np.fliplr([precision])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "recall = np.fliplr([recall])[0]  #so the array is increasing (you won't get negative AUC)\n",
    "AUC_prec_rec = np.trapz(precision,recall)\n",
    "print(\"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec))\n",
    "prec_rec_curve = plt.figure()\n",
    "plt.plot(recall,precision,'-',label='Area Under the Curve (AUC = %0.4f)' % AUC_prec_rec)\n",
    "plt.title('Precision - Recall curve')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(path_experiment+\"Precision_recall.png\")\n",
    "\n",
    "#Confusion matrix\n",
    "threshold_confusion = 0.5\n",
    "print(\"\\nConfusion matrix:  Custom threshold (for positive) of \" +str(threshold_confusion))\n",
    "y_pred = np.empty((y_scores.shape[0]))\n",
    "for i in range(y_scores.shape[0]):\n",
    "    if y_scores[i]>=threshold_confusion:\n",
    "        y_pred[i]=1\n",
    "    else:\n",
    "        y_pred[i]=0\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print(confusion)\n",
    "accuracy = 0\n",
    "if float(np.sum(confusion))!=0:\n",
    "    accuracy = float(confusion[0,0]+confusion[1,1])/float(np.sum(confusion))\n",
    "print(\"Global Accuracy: \" +str(accuracy))\n",
    "specificity = 0\n",
    "if float(confusion[0,0]+confusion[0,1])!=0:\n",
    "    specificity = float(confusion[0,0])/float(confusion[0,0]+confusion[0,1])\n",
    "print(\"Specificity: \" +str(specificity))\n",
    "sensitivity = 0\n",
    "if float(confusion[1,1]+confusion[1,0])!=0:\n",
    "    sensitivity = float(confusion[1,1])/float(confusion[1,1]+confusion[1,0])\n",
    "print(\"Sensitivity: \" +str(sensitivity))\n",
    "precision = 0\n",
    "if float(confusion[1,1]+confusion[0,1])!=0:\n",
    "    precision = float(confusion[1,1])/float(confusion[1,1]+confusion[0,1])\n",
    "print(\"Precision: \" +str(precision))\n",
    "\n",
    "#Jaccard similarity index\n",
    "jaccard_index = jaccard_similarity_score(y_true, y_pred, normalize=True)\n",
    "print(\"\\nJaccard similarity score: \" +str(jaccard_index))\n",
    "\n",
    "#F1 score\n",
    "F1_score = f1_score(y_true, y_pred, labels=None, average='binary', sample_weight=None)\n",
    "print(\"\\nF1 score (F-measure): \" +str(F1_score))\n",
    "\n",
    "#Save the results\n",
    "file_perf = open(path_experiment+'performances.txt', 'w')\n",
    "file_perf.write(\"Area under the ROC curve: \"+str(AUC_ROC)\n",
    "                + \"\\nArea under Precision-Recall curve: \" +str(AUC_prec_rec)\n",
    "                + \"\\nJaccard similarity score: \" +str(jaccard_index)\n",
    "                + \"\\nF1 score (F-measure): \" +str(F1_score)\n",
    "                +\"\\n\\nConfusion matrix:\"\n",
    "                +str(confusion)\n",
    "                +\"\\nACCURACY: \" +str(accuracy)\n",
    "                +\"\\nSENSITIVITY: \" +str(sensitivity)\n",
    "                +\"\\nSPECIFICITY: \" +str(specificity)\n",
    "                +\"\\nPRECISION: \" +str(precision)\n",
    "                )\n",
    "file_perf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50,60):\n",
    "    image_gray(pred_images[i+100].squeeze(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
