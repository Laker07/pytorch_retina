{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "\n",
    "from skimage.filters import sobel\n",
    "from skimage.morphology import watershed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms as tf\n",
    "\n",
    "import h5py\n",
    "\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "# from sklearn import preprocessing\n",
    "from skimage import transform\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from imgaug import augmenters as iaaot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, '../lib/')\n",
    "# from help_functions import *\n",
    "\n",
    "# #function to obtain data for training/testing (validation)\n",
    "# from extract_patches import get_data_training\n",
    "# sys.path.insert(0, '../lib/networks/')\n",
    "\n",
    "from preprocessing import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train images/masks shape:\n",
      "(20, 1, 565, 565)\n",
      "train images range (min-max): 0.0 - 1.0\n",
      "train masks are within 0-1\n",
      "\n",
      "patches per full image: 9500\n",
      "\n",
      "train PATCHES images/masks shape:\n",
      "(190000, 1, 48, 48)\n",
      "train PATCHES images range (min-max): 0.00784313725490196 - 1.0\n",
      "(190000, 1, 48, 48)\n",
      "48 48 1\n",
      "......DONE......\n",
      "old shape:  (190000, 2304)\n",
      "new shape:  (190000, 2304, 2)\n"
     ]
    }
   ],
   "source": [
    "train_img, label_img = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_subimgs = 190000\n",
    "indices = list(range(N_subimgs))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "val_size = 1/10\n",
    "split = np.int_(np.floor(val_size * N_subimgs))\n",
    "\n",
    "train_idxs = indices[split:]\n",
    "val_idxs = indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eye_dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,preprocessed_images, train=True, label=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_file(string): path to text file\n",
    "            root_dir(string): directory with all train images\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.images = preprocessed_images\n",
    "        if self.train:\n",
    "            self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        img = np.zeros_like(image, dtype=np.float32)\n",
    "        \n",
    "        img += image\n",
    "        label = None\n",
    "        if self.train:\n",
    "            label = self.label[idx]\n",
    "#             msk = np.zeros((2,48,48), dtype=np.long)\n",
    "#             msk[1] = label\n",
    "#             msk[0] = 1-label\n",
    "            \n",
    "#             msk += label\n",
    "            return (img, label)\n",
    "        return img\n",
    "\n",
    "eye_dataset_train = eye_dataset(train_img[train_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[train_idxs])\n",
    "\n",
    "eye_dataset_val = eye_dataset(train_img[val_idxs], \n",
    "                                      train=True, \n",
    "                                      label=label_img[val_idxs])\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=eye_dataset_train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=eye_dataset_val, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def compute_qkv(chanIn,filters, qkv,layer_type='SAME'):\n",
    "    '''\n",
    "    Args:\n",
    "        inputs: a Tensor with shape [batch,channels, h, w]\n",
    "        total_key_filters: an integer\n",
    "        total_value_filters: and integer\n",
    "        layer_type: String, type of this layer -- SAME, DOWN, UP\n",
    "\n",
    "    Returns:\n",
    "        q: [batch, _h, _w, total_key_filters] tensor\n",
    "        k: [batch, h, w, total_key_filters] tensor\n",
    "\t\tv: [batch, h, w, total_value_filters] tensor\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "#     print('compute_qkv .........')\n",
    "    if qkv == 'q':\n",
    "        # linear transformation for q , filters = key_filters\n",
    "        if layer_type == 'SAME':\n",
    "            qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True,padding=0)\n",
    "        elif layer_type == 'DOWN':\n",
    "            qkv = nn.Conv2d(chanIn, filters, 3, 2, bias=True, padding =1)\n",
    "        elif layer_type == 'UP':\n",
    "            qkv = nn.ConvTranspose2d(chanIn, filtersx, 3, 2, bias=True, padding=1)\n",
    "    \n",
    "    if qkv == 'k':\n",
    "        # linear transformation for k\n",
    "        qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True, padding=0)\n",
    "\n",
    "    if qkv =='v':\n",
    "        # linear transformation for k, value filtesr\n",
    "        qkv = nn.Conv2d(chanIn, filters, 1, 1, bias=True, padding=0)\n",
    "\n",
    "    return qkv\n",
    "\n",
    "\n",
    "def split_heads(x, num_heads):\n",
    "    \"\"\"Split channels (last dimension) into multiple heads (becomes dimension 1).\n",
    "\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, h, w, channels]\n",
    "        num_heads: an integer\n",
    "\n",
    "    Returns:\n",
    "        a Tensor with shape [batch, num_heads, h, w, channels / num_heads]\n",
    "    \"\"\"\n",
    "    \n",
    "    return split_last_dimension(x, num_heads).permute(0,4,1,2,3)#permute(0,3,1,2,4)\n",
    "\n",
    "def split_last_dimension(x,n):\n",
    "    \"\"\"Reshape x so that the last dimension becomes two dimensions.\n",
    "    The first of these two dimensions is n.\n",
    "    Args:\n",
    "        x: a Tensor with shape [..., m]\n",
    "        n: an integer.\n",
    "    Returns:\n",
    "        a Tensor with shape [..., n, m/n]\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    chunk_size = int(x.shape[3]/ n)\n",
    "    ret = torch.unsqueeze(x,4)\n",
    "    \n",
    "    ret = torch.cat(ret.split(split_size=chunk_size, dim=3),4)#.permute(0,1,2,4,3)\n",
    "#     print('split', ret.shape)\n",
    "#     ret.view(new_shape)\n",
    "#     print('split view ', ret.shape)\n",
    "    return ret\n",
    "\n",
    "def global_attention(q, k, v, chan_z):\n",
    "    \"\"\"global self-attention.\n",
    "    Args:\n",
    "        q: a Tensor with shape [batch, heads, _d, _h, _w, channels_k]\n",
    "        k: a Tensor with shape [batch, heads, d, h, w, channels_k]\n",
    "        v: a Tensor with shape [batch, heads, d, h, w, channels_v]\n",
    "        name: an optional string\n",
    "    Returns:\n",
    "        a Tensor of shape [batch, heads, _d, _h, _w, channels_v]\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "#     print(new_shape)\n",
    "    # flatten q,k,v\n",
    "    q_new = flatten(q)\n",
    "    k_new = flatten(k)\n",
    "    v_new = flatten(v)\n",
    "\n",
    "    # attention\n",
    "    output = dot_product_attention(q_new, k_new, v_new, bias=None,\n",
    "                dropout_rate=0.5)\n",
    "#     print('outp ', output.shape)\n",
    "\n",
    "    # putting the representations back in the right place\n",
    "#     print('output before scatter', output.shape)\n",
    "    output = scatter(output, chan_z)\n",
    "\n",
    "    return output\n",
    "\n",
    "def dot_product_attention(q, k, v, bias,dropout_rate=0.0):\n",
    "    \"\"\"Dot-product attention.\n",
    "    Args:\n",
    "        q: a Tensor with shape [batch, heads, length_q, channels_k]\n",
    "        k: a Tensor with shape [batch, heads, length_kv, channels_k]\n",
    "        v: a Tensor with shape [batch, heads, length_kv, channels_v]\n",
    "        bias: bias Tensor\n",
    "        dropout_rate: a floating point number\n",
    "        name: an optional string\n",
    "    Returns:\n",
    "        A Tensor with shape [batch, heads, length_q, channels_v]\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # [batch, num_heads, length_q, length_kv]\n",
    "#     print(q.shape, k.transpose(2,3).shape)\n",
    "    logits = torch.matmul(q, k.transpose(2,3))\n",
    "\n",
    "    if bias is not None:\n",
    "        logits += bias\n",
    "\n",
    "    weights = F.softmax(logits)\n",
    "\n",
    "    # dropping out the attention links for each of the heads\n",
    "    weights = F.dropout(weights, dropout_rate)\n",
    "\n",
    "    return torch.matmul(weights, v)\n",
    "\n",
    "\n",
    "def reshape_range(tensor, i, j, shape):\n",
    "    \"\"\"Reshapes a tensor between dimensions i and j.\"\"\"\n",
    "\n",
    "    target_shape = tf.concat(\n",
    "            [tf.shape(tensor)[:i], shape, tf.shape(tensor)[j:]],\n",
    "            axis=0)\n",
    "\n",
    "    return tf.reshape(tensor, target_shape)\n",
    "\n",
    "\n",
    "\n",
    "def scatter(x, chn):\n",
    "    \"\"\"scatter x.\"\"\"\n",
    "    \n",
    "#     print('scatter shape in ', x.shape)\n",
    "\n",
    "    x = x.view(x.shape[0],chn,chn,x.shape[3],-1)\n",
    "#     print('scatter ', x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def flatten(x):\n",
    "    \"\"\"flatten x.\"\"\"\n",
    "    # [batch, heads, h,w, ch/h]\n",
    "    # [batch, heads, length, channels], length = d*h*w\n",
    "    \n",
    "    l  = x.shape[2] * x.shape[3]\n",
    "    # [batch, heads, length, channels], length = d*h*w\n",
    "    x = x.view(x.shape[0], x.shape[1], l,-1)\n",
    "#     print('flatten shape', x.shape)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def combine_heads(x):\n",
    "    \"\"\"Inverse of split_heads_3d.\n",
    "    Args:\n",
    "        x: a Tensor with shape [batch, num_heads, d, h, w, channels / num_heads]\n",
    "    Returns:\n",
    "        a Tensor with shape [batch, d, h, w, channels]\n",
    "    \"\"\"\n",
    "#  [0, 2, 3, 4, 1, 5]\n",
    "#     print('combine heads ,', x.shape)\n",
    "    return combine_last_two_dimensions(x)\n",
    "\n",
    "\n",
    "def combine_last_two_dimensions(x):\n",
    "    \"\"\"Reshape x so that the last two dimension become one.\n",
    "    Args:\n",
    "        x: a Tensor with shape [..., a, b]\n",
    "    Returns:\n",
    "        a Tensor with shape [..., a*b]\n",
    "    \"\"\"\n",
    "\n",
    "#     old_shape = x.get_shape().dims\n",
    "#     a, b = old_shape[-2:]\n",
    "#     new_shape = old_shape[:-2] + [a * b if a and b else None]\n",
    "\n",
    "#     ret = tf.reshape(x, tf.concat([tf.shape(x)[:-2], [-1]], 0))\n",
    "#     ret.set_shape(new_shape)\n",
    "    \n",
    "    x = x.contiguous().view(x.shape[0],x.shape[1], x.shape[2], -1)\n",
    "#     print('combine last two ', x.shape)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"This script defines 3D different multi-head attention layers.\n",
    "(12,12,12,.5,2,False)\n",
    "\"\"\"\n",
    "class MultiHeadAttention_(nn.Module):\n",
    "    def __init__(self,\n",
    "                 chanIn,\n",
    "                 output_filters,\n",
    "                 total_key_filters,\n",
    "                 total_value_filters,                 \n",
    "                 num_heads,\n",
    "                 layer_type='SAME'):\n",
    "        super(MultiHeadAttention_, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        inputs: channels first as input [batch, chn, h,w]\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \"\"\"3d Multihead scaled-dot-product attention with input/output transformations.\n",
    "\n",
    "        Args:\n",
    "            inputs: a Tensor with shape [batch, h, w, channels]\n",
    "            \n",
    "            total_key_filters: an integer. Note that queries have the same number \n",
    "                of channels as keys\n",
    "            total_value_filters: an integer\n",
    "            output_depth: an integer\n",
    "            num_heads: an integer dividing total_key_filters and total_value_filters\n",
    "            layer_type: a string, type of this layer -- SAME, DOWN, UP\n",
    "            name: an optional string\n",
    "        Returns:\n",
    "            A Tensor of shape [batch, _d, _h, _w, output_filters]\n",
    "\n",
    "        Raises:\n",
    "            ValueError: if the total_key_filters or total_value_filters are not divisible\n",
    "                by the number of attention heads.\n",
    "        \"\"\"\n",
    "\n",
    "        if total_key_filters % num_heads != 0:\n",
    "            raise ValueError(\"Key depth (%d) must be divisible by the number of \"\n",
    "                            \"attention heads (%d).\" % (total_key_filters, num_heads))\n",
    "        if total_value_filters % num_heads != 0:\n",
    "            raise ValueError(\"Value depth (%d) must be divisible by the number of \"\n",
    "                            \"attention heads (%d).\" % (total_value_filters, num_heads))\n",
    "        if layer_type not in ['SAME', 'DOWN', 'UP']:\n",
    "            raise ValueError(\"Layer type (%s) must be one of SAME, \"\n",
    "                            \"DOWN, UP.\" % (layer_type))\n",
    "            \n",
    "        \n",
    "        '''\n",
    "        inputs: [batch, chn, h,w]\n",
    "        output: [batch, chn, h,w]\n",
    "        next step, permute to [batch, h,w, chn]\n",
    "        '''\n",
    "        self.q = compute_qkv(chanIn, total_key_filters,'q',layer_type='SAME')\n",
    "        self.k = compute_qkv(chanIn, total_key_filters,'k',layer_type='SAME')\n",
    "        self.v = compute_qkv(chanIn, total_value_filters,'v',layer_type='SAME')\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.total_key_filters = total_key_filters\n",
    "        self.total_value_filters = total_value_filters\n",
    "        \n",
    "        self.conv = nn.Conv2d(total_key_filters, output_filters, 1,1,bias=True)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "#         print('x before q shape in: ', x.shape)\n",
    "        #[batch, chn, h,w]\n",
    "        q = self.q(x)\n",
    "#         print('q shape in: ', q.shape)\n",
    "        k = self.k(x)\n",
    "#         print('k shape in: ', k.shape)\n",
    "        v = self.v(x)\n",
    "#         print('v shape in: ', v.shape)\n",
    "        \n",
    "        #permute to set [batch, h,w, chn]\n",
    "        q = q.permute(0,2,3,1)\n",
    "#         print('q after permute ', q.shape)\n",
    "        k = k.permute(0,2,3,1)\n",
    "        v = v.permute(0,2,3,1)\n",
    "        \n",
    "#         print('q shape before split ' ,q.shape[3], q.shape)\n",
    "        q = split_heads(q,self.num_heads)    \n",
    "#         print('q shape after split ' ,q.shape[3], q.shape)\n",
    "        k = split_heads(k,self.num_heads)\n",
    "        v = split_heads(v,self.num_heads)\n",
    "        #after split [batch, heads, h,w,k/h]\n",
    "#         print('q split ', q.shape)\n",
    "\n",
    "        #normalize\n",
    "        key_filters_per_head = self.total_key_filters // self.num_heads\n",
    "        q *= key_filters_per_head**-0.5\n",
    "        \n",
    "        att = global_attention(q,k,v, q.shape[2])\n",
    "#         print('out of attt : ', att.shape)\n",
    "\n",
    "\n",
    "\n",
    "        x = combine_heads(att)\n",
    "        \n",
    "#         print('LAST shape in, ' ,x.shape)\n",
    "        x = x.permute(0,3,1,2)\n",
    "#         print('LAST shape in, ' ,x.shape)\n",
    "        x = self.conv(x)\n",
    "#         print('out of att', x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_relu(chanIn, chanOut, ks = 3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(chanIn),\n",
    "        nn.ReLU6(inplace=True),\n",
    "        nn.Conv2d(chanIn, chanOut, ks, stride, padding=1),\n",
    "        \n",
    "    )\n",
    "\n",
    "class Att_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_block_1 = bn_relu(1, 1, 3,1)\n",
    "        self.input_block_2 = bn_relu(1, 1, 3,1)\n",
    "        self.input_1x1 = nn.Conv2d(1,32,1,1)\n",
    "        \n",
    "        self.down1_block_1 = bn_relu(32, 64, 3,2)\n",
    "        self.down1_block_2 = bn_relu(64,64, 3,1)\n",
    "        self.down1_1x1 = nn.Conv2d(32, 64, 1,2)\n",
    "        \n",
    "        self.down2_block_1 = bn_relu(64, 128, 3,2)\n",
    "        self.down2_block_2 = bn_relu(128, 128, 3,1)\n",
    "        self.down2_1x1 = nn.Conv2d(64, 128, 1,2)\n",
    "        \n",
    "        self.mid = MultiHeadAttention_(128,128,32,32,4)\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2)\n",
    "        self.up2_1 = nn.Conv2d(128,64,3,1,1)\n",
    "#         self.up2m = MultiHeadAttention_(64,64,12,12,4)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.up1 = nn.Upsample(scale_factor=2)\n",
    "        self.up1_1 = nn.Conv2d(64,32,3,1,1)\n",
    "#         self.up1m = MultiHeadAttention_(32,32,12,12,4)\n",
    "#         self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.out = nn.Conv2d(32,2,3,1,1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('input shape ', x.shape)\n",
    "        x_top = x\n",
    "        x = self.input_block_1(x)\n",
    "        x = self.input_block_2(x)\n",
    "        x = torch.add(x_top,x) #res1\n",
    "#         print('x before 1x1 shape: ',x.shape)\n",
    "        x_res1 = self.input_1x1(x)\n",
    "#         print('xres1  ', x_res1.shape)\n",
    "        \n",
    "        x_l = self.down1_1x1(x_res1)\n",
    "        x = self.down1_block_1(x_res1)\n",
    "        x = self.down1_block_2(x)\n",
    "        x_res2 = torch.add(x_l, x) \n",
    "        x_l = self.down2_1x1(x_res2)\n",
    "        x = self.down2_block_1(x_res2)\n",
    "        x = self.down2_block_2(x)\n",
    "        x_res3 = torch.add(x_l,x) \n",
    "        \n",
    "#       res3 shape  torch.Size([32, 128, 12, 12])\n",
    "        x = self.mid(x_res3)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = self.up2_1(x)\n",
    "        \n",
    "        x = torch.add(x, x_res2)\n",
    "        \n",
    "        x = self.up1(x)\n",
    "        x = self.up1_1(x)\n",
    "        x = torch.add(x, x_res1)\n",
    "        x = self.out(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], x.shape[1],-1)\n",
    "        x = x.permute(0,2,1)\n",
    "#         print('out : ', x.shape)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = Att_Net()\n",
    "model.cuda()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=.1,\n",
    "                             weight_decay=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_block_1.0.weight torch.Size([1])\n",
      "input_block_1.0.bias torch.Size([1])\n",
      "input_block_1.2.weight torch.Size([1, 1, 3, 3])\n",
      "input_block_1.2.bias torch.Size([1])\n",
      "input_block_2.0.weight torch.Size([1])\n",
      "input_block_2.0.bias torch.Size([1])\n",
      "input_block_2.2.weight torch.Size([1, 1, 3, 3])\n",
      "input_block_2.2.bias torch.Size([1])\n",
      "input_1x1.weight torch.Size([32, 1, 1, 1])\n",
      "input_1x1.bias torch.Size([32])\n",
      "down1_block_1.0.weight torch.Size([32])\n",
      "down1_block_1.0.bias torch.Size([32])\n",
      "down1_block_1.2.weight torch.Size([64, 32, 3, 3])\n",
      "down1_block_1.2.bias torch.Size([64])\n",
      "down1_block_2.0.weight torch.Size([64])\n",
      "down1_block_2.0.bias torch.Size([64])\n",
      "down1_block_2.2.weight torch.Size([64, 64, 3, 3])\n",
      "down1_block_2.2.bias torch.Size([64])\n",
      "down1_1x1.weight torch.Size([64, 32, 1, 1])\n",
      "down1_1x1.bias torch.Size([64])\n",
      "down2_block_1.0.weight torch.Size([64])\n",
      "down2_block_1.0.bias torch.Size([64])\n",
      "down2_block_1.2.weight torch.Size([128, 64, 3, 3])\n",
      "down2_block_1.2.bias torch.Size([128])\n",
      "down2_block_2.0.weight torch.Size([128])\n",
      "down2_block_2.0.bias torch.Size([128])\n",
      "down2_block_2.2.weight torch.Size([128, 128, 3, 3])\n",
      "down2_block_2.2.bias torch.Size([128])\n",
      "down2_1x1.weight torch.Size([128, 64, 1, 1])\n",
      "down2_1x1.bias torch.Size([128])\n",
      "mid.q.weight torch.Size([32, 128, 1, 1])\n",
      "mid.q.bias torch.Size([32])\n",
      "mid.k.weight torch.Size([32, 128, 1, 1])\n",
      "mid.k.bias torch.Size([32])\n",
      "mid.v.weight torch.Size([32, 128, 1, 1])\n",
      "mid.v.bias torch.Size([32])\n",
      "mid.conv.weight torch.Size([128, 32, 1, 1])\n",
      "mid.conv.bias torch.Size([128])\n",
      "bn.weight torch.Size([128])\n",
      "bn.bias torch.Size([128])\n",
      "up2_1.weight torch.Size([64, 128, 3, 3])\n",
      "up2_1.bias torch.Size([64])\n",
      "up1_1.weight torch.Size([32, 64, 3, 3])\n",
      "up1_1.bias torch.Size([32])\n",
      "out.weight torch.Size([2, 32, 3, 3])\n",
      "out.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for n, p in model.named_parameters():\n",
    "    print(n, p.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:122: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:....... 0.2502899529233932\n",
      "Validation loss:..... 0.22556899232093733\n",
      "NEW BEST Loss: 0.22556899232093733 ........old best:99999\n",
      "\n",
      "\n",
      "EPOCH:  2\n",
      "Training loss:....... 0.2197732792464559\n",
      "Validation loss:..... 0.22255984084172684\n",
      "NEW BEST Loss: 0.22255984084172684 ........old best:0.22556899232093733\n",
      "\n",
      "\n",
      "EPOCH:  3\n",
      "Training loss:....... 0.22296283318811727\n",
      "Validation loss:..... 0.26073129841374226\n",
      "\n",
      "EPOCH:  4\n",
      "Training loss:....... 0.22228942547149644\n",
      "Validation loss:..... 0.22401633923904662\n",
      "\n",
      "EPOCH:  5\n",
      "Training loss:....... 0.22170294108549635\n",
      "Validation loss:..... 0.2204093352110699\n",
      "NEW BEST Loss: 0.2204093352110699 ........old best:0.22255984084172684\n",
      "\n",
      "\n",
      "EPOCH:  6\n",
      "Training loss:....... 0.22108966158424131\n",
      "Validation loss:..... 0.23112892040901312\n",
      "\n",
      "EPOCH:  7\n",
      "Training loss:....... 0.22048931363077756\n",
      "Validation loss:..... 0.22219244513648126\n",
      "\n",
      "EPOCH:  8\n",
      "Training loss:....... 0.22061690927377184\n",
      "Validation loss:..... 0.21760794791308316\n",
      "NEW BEST Loss: 0.21760794791308316 ........old best:0.2204093352110699\n",
      "\n",
      "\n",
      "EPOCH:  9\n",
      "Training loss:....... 0.22064461266833865\n",
      "Validation loss:..... 0.2211622369510156\n",
      "\n",
      "EPOCH:  10\n",
      "Training loss:....... 0.22003160219104168\n",
      "Validation loss:..... 0.21968620217809773\n",
      "\n",
      "EPOCH:  11\n",
      "Training loss:....... 0.21971375032322493\n",
      "Validation loss:..... 0.21805100464058244\n",
      "\n",
      "EPOCH:  12\n",
      "Training loss:....... 0.21956832325476372\n",
      "Validation loss:..... 0.2279277001366471\n",
      "\n",
      "EPOCH:  13\n",
      "Training loss:....... 0.21953252910965396\n",
      "Validation loss:..... 0.2178151197606064\n",
      "\n",
      "EPOCH:  14\n",
      "Training loss:....... 0.21961216166793943\n",
      "Validation loss:..... 0.22227169437841934\n",
      "\n",
      "EPOCH:  15\n",
      "Training loss:....... 0.21927158443067601\n",
      "Validation loss:..... 0.2270981443149072\n",
      "\n",
      "EPOCH:  16\n",
      "Training loss:....... 0.21934953183604927\n",
      "Validation loss:..... 0.2303337106098631\n",
      "\n",
      "EPOCH:  17\n",
      "Training loss:....... 0.21956152831968254\n",
      "Validation loss:..... 0.24246450706764502\n",
      "\n",
      "EPOCH:  18\n",
      "Training loss:....... 0.21939212884642406\n",
      "Validation loss:..... 0.22083663107570173\n",
      "\n",
      "EPOCH:  19\n",
      "Training loss:....... 0.21957298987796028\n",
      "Validation loss:..... 0.2186576948322431\n",
      "\n",
      "EPOCH:  20\n",
      "Training loss:....... 0.2194538646249357\n",
      "Validation loss:..... 0.22334700250866438\n",
      "\n",
      "EPOCH:  21\n",
      "Training loss:....... 0.2193070772957213\n",
      "Validation loss:..... 0.22021152264742738\n",
      "\n",
      "EPOCH:  22\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2d2034c1ca2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "minLoss = 99999\n",
    "maxValacc = -99999\n",
    "for epoch in range(100):\n",
    "    print('EPOCH: ',epoch+1)\n",
    "#     train_losses = []\n",
    "#     val_losses = []    \n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    count = 0\n",
    "    for images, labels in train_loader:    \n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "#         print(labels.type())\n",
    "    \n",
    "#         print(images.shape)\n",
    "        outputs = model(images) \n",
    "#         print(outputs.shape,outputs)\n",
    "#         print(labels.shape,labels)  \n",
    "#         print(torch.max(labels, 1)[1])\n",
    "#         print(images.shape, labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "#         train_acc.append(accuracy(outputs, labels))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        count +=1\n",
    "    \n",
    "    print('Training loss:.......', running_loss/count)\n",
    "#     print('Training accuracy:...', np.mean(train_acc))\n",
    "    mean_train_losses.append(running_loss/count)\n",
    "        \n",
    "    model.eval()\n",
    "    count = 0\n",
    "    val_running_loss = 0.0\n",
    "    for images, labels in val_loader:\n",
    "#         labels = labels.squeeze()\n",
    "        images = Variable(images.cuda())\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "#         val_acc.append(accuracy(outputs, labels))\n",
    "        val_running_loss += loss.item()\n",
    "        count +=1\n",
    "\n",
    "    mean_val_loss = val_running_loss/count\n",
    "    print('Validation loss:.....', mean_val_loss)\n",
    "    \n",
    "#     print('Training accuracy:...', np.mean(train_acc))\n",
    "#     print('Validation accuracy..', np.mean(val_acc))\n",
    "    \n",
    "    mean_val_losses.append(mean_val_loss)\n",
    "    \n",
    "#     mean_train_acc.append(np.mean(train_acc))\n",
    "    \n",
    "#     val_acc_ = np.mean(val_acc)\n",
    "#     mean_val_acc.append(val_acc_)\n",
    "    \n",
    "   \n",
    "    if mean_val_loss < minLoss:\n",
    "        torch.save(model.state_dict(), 'attention_torch_190k.pth' )\n",
    "        print(f'NEW BEST Loss: {mean_val_loss} ........old best:{minLoss}')\n",
    "        minLoss = mean_val_loss\n",
    "        print('')\n",
    "        \n",
    "#     if val_acc_ > maxValacc:\n",
    "#         torch.save(model.state_dict(), 'res/cam_40/best_acc_norm_10x10.pth' )\n",
    "#         print(f'NEW BEST Acc: {val_acc_} ........old best:{maxValacc}')\n",
    "#         maxValacc = val_acc_\n",
    "    \n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
